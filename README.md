# ScratchGPT
This project was built to understand how GPT-2 works internally by implementing it from scratch. These are the key concepts learned:

ğŸ”¤ Tokenization (GPT-2 BPE)

Text is converted to subword tokens using Byte Pair Encoding.

Token IDs â†’ embeddings â†’ model.

Outputs are token IDs converted back to text.

ğŸ¯ Next-Token Prediction (x vs y)

Input sequence x.

Target sequence y = x shifted by 1.

Model learns: predict the next token at every step.

ğŸ”¢ Embeddings

Token embeddings + positional embeddings.

Combine to form the input to the transformer.

ğŸ§  Self-Attention

Compute Q, K, V from embeddings.

Attention scores = Q Â· Káµ€ / sqrt(d).

Softmax â†’ weights â†’ weighted sum of V.

Allows model to focus on relevant previous tokens.

ğŸ§© Multi-Head Attention

Several attention heads in parallel.

Concatenate head outputs â†’ linear projection.

Helps model learn different relationships at once.

ğŸ— Transformer Block

Multi-head attention

Feed-forward network (MLP)

Residual connections

LayerNorm

Stacked to build the full model.

ğŸ”¥ Causal Masking

Ensures each token can only attend to previous tokens.

Enforces autoregressive generation.

ğŸ› Optimization

AdamW optimizer

Warmup + cosine LR decay

Gradient clipping

Cross entropy loss on logits vs targets

âš¡ Training Pipeline

Batch and sequence length configuration

Dataloader producing (x, y) pairs

Forward â†’ loss â†’ backward â†’ optimizer step

Periodic evaluation and checkpointing

ğŸ¤– Text Generation

Autoregressive sampling loop

Temperature, top-k, top-p sampling

Using the trained model to produce text token-by-token
